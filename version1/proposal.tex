
This section is devoted to describe our proposal, the \textit{Archived Variable Space Diversity \MOEA{} based on Decomposition} (\AVSDMOEAD{}).
%
The main novelty and motivation behind \AVSDMOEAD{} is the incorporation of an explicit management of the diveristy on variable space
with the aim of improving the behaviour in terms of objective space metrics specially in long-term executions, which is the
environment where diversity-aware techniques have excelled.
%
Although \AVSDMOEAD{} is inspired in \MOEAD{}, it was simplified so in some ways it resembles more mature
decomposition-based \MOEAS{}, such as \MOGA{}.
%
For instance, the notion of subproblem neighborhood is not used and the dynamic resource allocation usually applied in modern variants
of \MOEAD{} is deactivated.
%
The main reason for the simplification is to show that even a simple \MOEA{} incorporating our design principles can
improve further more complex state-of-the-art algorithms.
%
In addition, \AVSDMOEAD{} incorporates an external archive, whose density estimator is guided by the principle of the R2-indicator~\cite{trautmann2013r2}.

Our proposal decomposes the \MOP{} in several single-objective problems.
%
Notwithstanding that any scalarization approach can be employed, our strategy applies the achievement scalarizing function (\ASF{}), which has shown
some of the most effective results in recent years~\cite{deb2013evolutionary, hernandez2015improved}.
%
Let $\lambda_1, ..., \lambda_N$ be a set of weight vectors and $z^*$ a reference vector,
the \MOP{} is decomposed into $N$ scalar optimization sub-problems as shown in (\ref{eqn:approach}).
%

\begin{equation}\label{eqn:approach}
\displaystyle{
 g^{te}(x| \lambda_j, z^*) = \max_{ 1 \leq i \leq M} \left \{ \frac{ | f_i(x) - z_i^*|}{\lambda_{j,i}} \right \} 
}
\end{equation}

\begin{algorithm}[!t]
\algsetup{linenosize=\tiny}
        \caption{Main procedure of \AVSDMOEAD{}}
        \begin{small}
\begin{algorithmic}[1]
        \STATE \textbf{Initialization}: Generate an initial population $P^0$ with $N$ individuals \label{alg_1:1}
        \STATE Let $\lambda = \{\lambda_1, ..., \lambda_N \}$ be a set of evenly spread weight vectors \label{alg_1:2}
        \STATE \textbf{Evaluation}: Evaluate each individual in $P^0$ and assemble the reference vector $z^*$ with the best objective values \label{alg_1:3}
        \STATE Assign $t=0$ \label{alg_1:4}
        \WHILE{ (not stopping criterion)  } \label{alg_1:5}
           \FOR{ each individual $P_i^t \in P^t$} \label{alg_1:6}
    %           \STATE \textbf{Mating selection}: Select randomly three indexes ($r_1 \neq r_2 \neq r_3 \neq i$) from the entire population. \label{alg_1:7}
               \STATE \textbf{DE variation}: Generate solution $Q^t_{i}$ by applying DE/rand/1/bin using $P_{i}^t$ as target vector \label{alg_1:8}
							 \STATE \textbf{Mutation}: Apply polynomial mutation to $Q^t_{i}$ with probability $p_m$
               \STATE \textbf{Evaluation}: Evaluate the new individual $Q^t_{i}$ and update the reference vector $z^*$ with the best objective values. \label{alg_1:9}
           \ENDFOR \label{alg_1:10}
           \STATE \textbf{Survivor selection}: Generate $P^{t+1}$ by applying the replacement scheme described in  Algorithm \ref{alg:replacement}, using $P^t$, $Q^t$, $\lambda$ and $z^*$ as input \label{alg_1:11}
	   \STATE \textbf{Update Archive}: Update $A^{t+1}$ using $Q^t$ following the R2-indicator criterion.
           \STATE $t=t+1$ \label{alg_1:12}
        \ENDWHILE \label{alg_1:13}
        \end{algorithmic}
        \end{small}
\label{alg:vsd-moead}
\end{algorithm}


The main novelty of \AVSDMOEAD{} appears in the  survivor selection scheme.
%
Following some of the most successful single-objective diversity-aware algorithms~\cite{segura2016improving}, the 
replacement strategy relates the degree of diversity on variable space to the stopping criterion
and elapsed generations.
%
The aim of this relation is to gradually bias the search from exploration to explotation as the
optimization evolves.
%
In particular, the diversity is explicitly promoted in a decreasing way until half of total generations. 
%
Then, in the remaining generations \AVSDMOEAD{} has a similar behavior than most popular
\MOEAS{}, i.e. the diversity on the variable space is not considered explicitly.

The main procedure of \AVSDMOEAD{} is shown in Algorithm~\ref{alg:vsd-moead}.
%
Its general template is quite standard.
%
The mating and variation components are similar to those used in typical \MOEAS{}.
%
Particularly, in the $t$ generation, the population $P^t$ is used to generate
the offspring $Q^t$ with $N$ individuals by randomly selecting at each step three indivuals
to apply the $DE/rand/1/bin$ operator.
%
Then, polynomial mutation is applied to the output of the $DE$ operator.
%
%TODO: c√≥mo se generaron los pesos?
As in most current \MOEAD{} variants, the initial population is generated randomly,
the number of weight vectors is equal to the population size,
and the reference vector $z^*$ used for \ASF{} is composed by the best attained 
objective values.
%
Finally, the survivor selection stage is applied.
%
This is quite different to traditional techniques, in the sense that $P^t$ and $Q^t$ are merged, meaning
that differently than in \MOEAD{} the position of each individual is not important, and a diversity-aware
selection is performed.
%
Since this is the novelty of the paper, its working operation is given in detail.

\begin{algorithm}[t]
\algsetup{linenosize=\tiny}
        \caption{Replacement Phase of \AVSDMOEAD{}}
\begin{small}
\begin{algorithmic}[1]
\STATE Input: $P^t$ (Parent of current generation), $Q^t$ (Offspring of current generation), $\lambda^t$ (a set of weight vectors) and $z^*$ (Reference vector)
        \STATE Output: $P^{t+1}$
        \STATE $R^t = P^t \cup Q^t$\label{alg_2:1} 
        \STATE $P^{t+1} = \emptyset$ \label{alg_2:2}
        \STATE $Penalized = \emptyset$ \label{alg_2:3}
	\STATE $\lambda^{t+1} = \emptyset$ \label{alg_2:4}
        \STATE $D^t = D_I - D_I * \frac{G_{Elapsed}}{0.5*G_{End}}$ \label{alg_2:5} 
        \WHILE{ $|P^{t+1}| <  N$} \label{alg_2:6}
            \STATE Compute $DCS$ in $R^t$ using $P^{t+1}$ as reference set \label{alg_2:7}
            \STATE Move the individuals in $R^t$ with $DCS < D^t$ to $Penalized$ \label{alg_2:8}
%	   \STATE Compute the diversity-contribution of each candidate $i \in R^t$ to the survivor set $P^{t+1}$\label{alg:7}
%	   \STATE Move the crowdest individuals from $R^t$ to $Penalized$; Those individuals whose diversity-contribution is less than the threshold $D^t$\label{alg:8}
                \IF{$R^t$ is empty} \label{alg_2:9}
                    \STATE Compute $DCS$ of each individual in $Penalized$ set employing $P^{t+1}$ as reference set \label{alg_2:10}
                    \STATE Move the individual in $Penalized$ with the largest $DCS$ to $R^t$ \label{alg_2:11}
%		    \STATE Compute the diversity-contribution of each individual in $Penalized$ to the survivor set $P^{t+1}$\label{alg:10}
%                    \STATE Move the most suitable individual from $Penalized$ to the survivor set $R^t$; the one with the highest diversity-contribution to $R^t$ \label{alg:11}
                \ENDIF \label{alg_2:12}
            \STATE Identify the non-penalized individual $R_i^t$ and the weight vector $\lambda_i^t$ with the best scalarizing function value according to $g^{te}(R_i^t | \lambda_j^t, z^*)$ \label{alg_2:13}
%	    \STATE $\displaystyle{ R_i^t, \lambda_i = \max_{k \in |R^t|, l \in |\Lambda|} g(R_k^t | \lambda_l, \mathbf{z})}$ 
	    \STATE Move the non-penalized individual $R_i^t$ to $P^{t+1}$ \label{alg_2:14}
            \STATE Move the associated weight vector $\lambda^t_j$ to $\lambda^{t+1}$ \label{alg_2:15}
        \ENDWHILE \label{alg_2:16}
        \RETURN $P^{t+1}$ \label{alg_2:17}
        \end{algorithmic}
\end{small}
\label{alg:replacement}
\end{algorithm}

\begin{algorithm}[!t]
\algsetup{linenosize=\tiny}
        \caption{R2-Indicator procedure}
        \begin{small}
\begin{algorithmic}[1]
	\STATE Input: $A^t$ (External archive at the current generation), $Q^t$ (Offspring of current generation), $\lambda^t$ (a set of weight vectors and $z^*$ (Reference vector)
	\STATE Output: $A^{t+1}$
	\STATE $R^t= A^t \cup Q^t$
	\STATE $A^{t+1} = \emptyset$
	\WHILE{ $|A^{t+1}| < N$ }
	\STATE $\forall x \in R^t : r(x) = R2(A^{t+1} \cup \{x\}; \lambda, z^*)$
	\STATE $x^* = argmin(r(x):x \in R^t)$ 
	\STATE $A^{t+1} = A^{t+1} \cup x$
	\STATE $R^t = R^t \setminus \{ x^* \}$ 
  	\ENDWHILE
        \end{algorithmic}
        \end{small}
\label{alg:r2_Indicator}
\end{algorithm}



%
\subsection{Novel Replacement Phase of \AVSDMOEAD{} }

The purpose of the replacement phase (see Algorithm~\ref{alg:replacement}) is to select the set of survivors of the next generation.
%
The survivor selection described in this work incorporates similar design principles to those applied in 
the diversity-aware single-objective optimizer \textsc{de-edm}~\cite{castillo2019differential}.
%
It operates as follows.
%
First, the parent and offspring populations are merged in a multi-set to establish the candidate set $R^t$ (line 3).
%
A key of the scheme is to promote the selection of individuals with a large enough contribution to diversity
on variable space.
%
Particularly, the contribution of an individual $x$ is calculated as $\displaystyle{\min_{p \in P^{t+1}}\ Distance(x, p)}$, 
where $P^{t+1}$ is the multi-set of the already picked survivors and the normalized Euclidean distance
specified in (\ref{eqn:distance}) is applied.
%
Note that in the pseudocode the tag \DCS{} (Distance to Closest Solution) is used to denote the contribution to diversity.

\begin{equation}\label{eqn:distance}
Distance(A, B) =   \left ( \frac{1}{D}  \sum_{i=1}^D \left ( \frac{A_i - B_i}{x_i^{(U)} - x_i^{(L)}} \right )^2  \right)^{1/2}
\end{equation}

In order to promote the selection of distant individuals, a threshold $D^t$ is dynamically calculated (line 7) and 
individuals with a $DCS$ value lower than the threshold are considered as undesirable individuals.
%
Note that the calculation of $D^t$ depends on an initial threshold value ($D_I$), which is a parameter of our proposal,
on the number of generations that have evolved ($G_{Elapsed}$) and on the stopping criterion ($G_{End}$), i.e., the number of
generations to evolve.
%
Particularly, the value is decreased linearly as generations evolve.
%
Since survivors with larger \DCS{} values, provoke exploration steps, while survivors with short \DCS{} values promote
intensification steps, this linear decrease promotes
a gradual transition from exploration to exploitation.
%
Also note that after $50\%$ of the total number of generations, the $D^t$ value is below 0, 
meaning that no penalties are applied and a more traditional strategy focused only on the objectives values
is used to perform the selection steps.

The strategy iteratively selects an individual from the candidate set ($R^t$) to enter the new population ($P^{t+1}$) until
it is filled with $N$ individuals (lines 8-16).
%
In particular, the aim is to select a proper individual for each weight vector but at the same time fulfilling
the condition imposed for the contribution to diversity on variable space.
%
In order to fulfil this last condition, non-selected individuals with a $DCS$
lower than $D^t$ are moved from $R^t$ to the $Penalized$ set (lines 9-10) and at each iteration
an individual belonging to $R^t$ is picked up to survive.
%
The set of weight vectors considered by our strategy are initially placed in $\lambda^{t}$.
%
At each iteration the individual in $R^t$ with the best scalarizing function for any of the weight vectors in
$\lambda^{t}$ is identified (line 14).
%
Then, such an individual is selected as a survivor (line 15) and the used weight vector is transferred to the
weight vector set of the next population (line 16).
%
Note that $N$ individuals are selected, meaning that each weight vector is used to select a single individual.
%
Also note that it might happen that $R^t$ is empty prior to selecting $N$ individuals.
%
This means that the diversity is lower than expected so
with the aim of increasing the exploration degree, the individual with the largest \DCS{} value in
the $Penalized$ set is selected to survive (lines 11 - 13).
