This document is intended to be a supplementary material of the main work titled ``AVSD-MOEA/D The Importance of Diversity in the Variable Space in the Design of Multi-objective Evolutionary Algorithms''.
%
Particularly, this extension seeks to complement the results discussed in the main document.
%
First, the main section \textit{Performance of \MOEAS{} in long-term executions} in terms of \textit{The Modified Inverted Generational Distance Plus} (\IGD{}+) is presented~\cite{ishibuchi2015modified}.
%
Second, a detailed analyses of the \textit{Test Problems with Bias Features} is driven.
%
The conclusions found in this document are quite similar to those obtained in the main document.


\input{tablas/Table_IGDP_2obj.tex}
\input{tablas/Tests_IGDP_2obj.tex}


\section{Comparison against State-of-the-art \MOEAS{} in long-term executions in terms of \IGD{}+ metric}

The \IGD{}+ indicator measures the average distance from each reference point to the nearest dominated region of the solution set.
%
Let us denote the reference point set as $Z = \{z_1, z_2, ..., z_{|Z|} \}$ where $z_i$ is a point in the objective space.
%
In this context the reference set can be seen as a discretization of the Pareto front.
%
Let us denote a solution set $A$ as $A = \{a_1, a_2, .., a_{|A|}\}$ where $a_j$ is a point in the objective space.
%
The \IGD{}+ indicator is defined as
\begin{equation}
	IGD+(A) = \frac{1}{|Z|} \sum_{i=1}^{|Z|} \min_{j=1}^{|Z|} d^+(z_i, a_j)
\end{equation}
where $d^+(z, a) = \sqrt{(max\{ \})}$ 

The basic idea in the \IGD{}+ is to calculate the distance from each reference point to the dominated region by a solution set.

One of the aims behind the design of \AVSDMOEAD{} is to profit from long-term executions.
%
Therefore, in this section we present the results attained by the different algorithms when setting 
the stopping criterion to $2.5 \times 10^7$ function evaluations.
%
Table~\ref{tab:StatisticsHV_2obj} shows the \HV{} ratios obtained for the benchmark functions with two objectives.
%
Note that the same results can be drawn with the IGD+ metric~\cite{ishibuchi2015modified} and can be inspected in the supplementary material.
%
For each method and problem, the best, mean and standard deviation of the \HV{} ratio values are reported.
%
Furthermore, in order to summarize the results attained by each method, the last row shows the mean for the whole set 
of problems.
%
For each test problem, the method that yielded the largest mean and those that were not statistically inferior to the 
best are shown in \textbf{boldface}.
%
Similarly, the method that yielded the best \HV{} value among all the runs is {\ul underlined}.
%
From here on, the methods shown in {\bf boldface} for a given problem are referred to as the winning methods.
%
\AVSDMOEAD{}, \RMOEA{}, \MOEADDE{}, \NSGAIII{} and \NSGAII{} belonged to the winning methods in 
$17$, $6$, $2$, $2$ and $0$ problems, respectively.
%
The superiority of \AVSDMOEAD{} is clear both in terms of this metric and in terms of the mean \HV{}.
%
Particularly, \AVSDMOEAD{} attained a value equal to $0.976$, while all the remaining methods attained values between
$0.931$ and $0.937$.
%
A careful inspection of the data shows that in those cases where \AVSDMOEAD{} loses, the difference with respect to the best 
method is low.
%
In fact, the difference between the mean \HV{} ratio attained by the best method and by \AVSDMOEAD{} is never greater than $0.1$.
%
However, in all the other methods, there were several problems where the distance with respect to the best approach
was greater than $0.1$.
%
Specifically, it happened in $4$, $4$, $4$ and $5$ problems for \RMOEA{}, \MOEADDE{}, \NSGAII{} and \NSGAIII{}, respectively.
%
This means that \AVSDMOEAD{} wins in most cases and that when it loses, the difference is always small.
%
Note also that in terms of standard deviation, \AVSDMOEAD{} yields much lower values than all the other algorithms, meaning
it is quite robust.


In order to better clarify these findings, pair-wise statistical tests were applied between each method tested in each test problem.
%
For the two-objective cases, Table~\ref{tab:Tests_HV_2obj} shows the number of times that each method statistically won 
(column~$\uparrow$), lost (column~$\downarrow$) or tied (column~$\leftrightarrow$).
%
The \textbf{Score} column shows the difference between the number of times that each method won and the number of times that each 
method lost.
%
Additionally, for each method $M$, we calculated the sum of the differences between the mean \HV{} ratio attained by the best method 
(the ones with the highest mean) and method $M$, for each problem where $M$ was not in the group of winning methods.
%
This value is shown in the \textit{Deterioration} column.
%
The data confirm that although \AVSDMOEAD{} loses in some pair-wise tests, the overall numbers of wins and 
losses clearly favor \AVSDMOEAD{}.
%
More importantly, the total deterioration is much lower in the case of \AVSDMOEAD{}, confirming that when \AVSDMOEAD{} loses, 
the differences are low.

\input{tablas/Table_IGDP_3obj.tex}
\input{tablas/Tests_IGDP_3obj.tex}


Tables~\ref{tab:StatisticsHV_3obj} and~\ref{tab:Tests_HV_3obj} shows the same information for the problems with three objectives.
%
In this case, the number of times that each method belonged to the winning groups were $17$, $2$, $0$, $0$ and $0$ 
for \AVSDMOEAD{}, \RMOEA{}, \MOEADDE{}, \NSGAIII{} and \NSGAII{}, respectively.
%
Thus, \AVSDMOEAD{} yielded quite superior results.
%
Considering the whole set of problems, \AVSDMOEAD{} obtained a much larger mean \HV{} ratio than the other ones.
%
Moreover, the difference between the mean \HV{} ratio obtained by the best method and by \AVSDMOEAD{} was never greater than $0.1$.
%
However, all the other methods exhibited a deterioration in excess of $0.1$ in several cases.
%
In particular, this happened in $2$, $2$, $2$ and $6$ problems for \MOEADDE{}, \RMOEA{}, \NSGAIII{} and \NSGAII{} respectively.
%
Remarkably, \AVSDMOEAD{} is quite superior in both the total deterioration and in the score generated from the pair-wise
statistical tests.
%
In fact, its deterioration for the entire problem set is just $0.006$.
%
Beating all the state-of-the-art algorithms in such a large number of problem benchmarks is a quite significant achievement, and shows
the robustness of \AVSDMOEAD{}.
%
Our results show that the superiority of \AVSDMOEAD{} persists, and even increases, when 
problems with three objective functions are considered.
%
For a better comprehension of the strenghts and weakness of the algorithms, in the Figure~\ref{fig:attainment} is shown the 50\% attainment surfaces for \WFG{}8 and \UF{}5.
%
An attainment surface approximation can be interpreted as the spatial region that is statistically attained among all the runs that were carried out by an algorithm~\cite{knowles2005summary, fonseca1996performance}.
%
In other words, it can be understood as \textit{the spatial region that is achieved by the $k\%$ among all the runs by one algorithm}.
%
The most challenging characteristic of these problems are that \WFG{}8 has strong dependencies among all the parameters, and \UF{}5 is a multi-modal biased problem whose Pareto optimal front is discrete and consists of $21$ points.
%
In both problems \AVSDMOEAD{} was the only one that converged adequately to the Pareto front at least 50\% among all the runs.
%
Even more, given that the standard deviation is too low it can be though that all the runs converged similarly well.
%



We can better understand the reasons behind the benefits of \AVSDMOEAD{} against the state-of-the-art \MOEAS{}
by analyzing the evolution of the \HV{} values and the diversity.
%
Note that in some \MOPS{}, variables can be classified into two types: 
distance variables and position variables.
%
A variable $x_i$ is a distance variable when for all $x$, modifying $x_i$ results in a new solution 
that dominates $x$, is equivalent to $x$, or is dominated by $x$.
%
Differently, if $x_i$ is a position variable, modifying $x_i$ in $x$ always results in a vector that is 
incomparable or equivalent to $x$~\cite{huband2006review}.
%
This is important because in some cases, \MOEAS{} do not maintain a large enough diversity in the distance
variables~\cite{castillo2017multi}, so analyzing the diversity trend for these kinds of variables provides an useful
insight into the dynamics of the population.

In order to show the behavior of the different schemes, we selected WFG5 and UF5.
%
They are complementary in the sense that in WFG5, all the Pareto solutions exhibit constant 
values for the distant variables, which is not the case in UF5.
%
Moreover, in UF5, the optimal regions are isolated in the variable 
space, meaning that more diversity is required.
%
For each algorithm, the diversity is calculated as the average Euclidean distance between individuals (\ADI{}) in the population 
by considering only the distance variables.
%
Figures~\ref{fig:WFG5_Diversity} and~\ref{fig:UF5_Diversity} show the evolution of the ADI (top) and the mean of \HV{} (bottom) 
for WFG5 and UF5, respectively.
%
In the WFG5 problem, the distance variables quickly converged to a small region 
in state-of-the-art \MOEAS{}.
%
Thus, the differential evolution operator loses it exploring power and as a result,
those \MOEAS{} were unable to significantly improve the quality of the approximations as the
evolution progresses.
%
By contrast, in the case of \AVSDMOEAD{}, the decrease in \ADI{} is quite linear until the midpoint of the execution, and
the increase in \HV{} is gradual.
%
The final \HV{} attained by \AVSDMOEAD{} is the largest one, which shows the important benefit
of gradually decreasing the diversity.

As expected, explicitly promoting diversity is also beneficial for problems with disconnected optimal regions.
%
As the data in Figure~\ref{fig:UF5_Diversity} show, the advantage of promoting diversity in the UF5 test 
problem is clear.
%
In this case, state-of-the-art algorithms maintain some degree of diversity in the distance variables for
the entire search.
%
However, a large degree of diversity is required to obtain the 21 optimal solutions, and these \MOEAS{} do not maintain
the required amount of diversity, and as a result, they miss many of the solutions.
%
In the case of \AVSDMOEAD{}, enforcing a large degree of diversity in the initial phases promotes more exploration, 
which makes it possible to find additional optimal regions.
%
Once these regions are located, they are not discarded, meaning that a larger level of diversity is maintained throughout the
execution.
%
This way, \AVSDMOEAD{} not only attained better \HV{} values for the first $10\%$ of the total function evaluations, but 
it also kept looking for promising regions.
%
In fact, its \HV{} values improved significantly until the midpoint of the execution period i.e., the final moment
when diversity was explicitly promoted.
%
Then, an additional increase was obtained due to intensification in the regions identified.
%
This analysis shows that the dynamic of the population depends on the problem at hand.
%
The behavior of \AVSDMOEAD{} with all the problems tested
was similar to those already presented.
%
Scenaries where the optimal regions consists of constant values for the distance variables behave like WFG5, whereas
the behavior in those cases where the optimal regions consist of non-constant values for the distance variables is
more similar to the UF5 case.
%
Note, however, that in these cases, different levels of diversity are required, so the behavior is not as homogeneous.






In order to better understand the importance of $D_I$, the entire set of benchmark problems was tested with different values
of $D_I$.
%
As in previous experiments, the stopping criterion was set to $2.5 \times 10^7$ function evaluations.
%
Since normalized distances are used, the maximum attainable distance between pairs of individuals is $1.0$.
%
Also note that setting $D_I$ to $0$ implies not promoting diversity in the variable space.
%
Thus, several values in this range were considered.
%
Specifically, the values $D_I = \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ were tested.
%
Figure~\ref{fig:Initial-distance-factor} shows the mean \HV{} ratio obtained for both the two-objective 
and the three-objective case with the $D_I$ values tested.
%
The \AVSDMOEAD{} performed worst when $D_I$ was set to $0$.
%
The \HV{} ratio quickly increased as higher $D_I$ values up to $0.2$ were used.
%
Larger values yielded quite similar performances.
%
Thus, a wide range of values (from $0.2$ to $1.0$) exhibited very good performance, 
meaning that the behavior of \AVSDMOEAD{} is quite robust.
%
Thus, properly setting this parameter is not a complex task.

In order to better understand the implications of $D_I$ on the dynamics of the population, Figure~\ref{fig:WFG9_Diversity}
shows, for \AVSDMOEAD{}, the evolution of diversity in the distance variables in the \textsc{wfg9} case for three different values of $D_I$.
%
When setting $D_I = 0$, the diversity is reduced quite quickly, which results in premature convergence.
%
The result is a hypervolume that is not too high.
%
However, when $D_I = 0.4$ and $D_I = 1$ are used, the loss of diversity is slowed down, and the resulting hypervolume is quite large.
%
Note that setting $D_I = 1$ promotes greater diversity, so the hypervolume increases slower than when
$D_I = 0.4$.
%
However, the degree of exploration in both cases is enough to yield high-quality solutions.
%
The behavior is quite similar in every problem, which explains the stability of the algorithms for
different values of $D_I$.
%
Note that for shorter periods, setting a proper $D_I$ value is probably much more important.
%
However, for long-term executions at least, practically any value higher than $0.2$ yields similar solutions,
which we regard as a highly positive feature.
\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{images/BIAS_Full-figure0.eps} \\
\caption{Mean of \HV{} values for eight \BTS{} problems (y-axis) against several biasses ratios (x-axis). The \BT{}2 problem is not taken into consideration due that it suffers of numerical stability.}\label{fig:BT}
\end{figure}

\section{On the Convergence of \MOEAS{} in Test Problems with Bias Features}

As pointed out in~\cite{li2016biased, deb1999multi, huband2006review}, the bias feature is one of the most challenging difficulties that \MOEAS{} might face.
%
Recently, the \BTS{} test problems were proposed to facilitate the study of the ability of \MOEAS{} for dealing with biases.
%
In this context bias means that small variations in the decision space around the Pareto set cause significant changes in vicinities of some Pareto front solutions~\cite{huband2006review}.
%
Particularly, those problems are built with transformations that induce position-related bias and distance-related bias.
%
While the former means that a small change on the position-related variables of one solution in the Pareto set projects a significant change along the Pareto front.
%
The later imposes that a small variation on the distance-related variables of one solution in the Pareto set causes a significant deterioration on the convergence towards the Pareto front.
%

In order, to analyze the capability of the \MOEAS{} to deal with bias features the \BTS{} problems are taken into account.
%
Specifically, this section analyses the sensitivity of the algorithms imposing several levels of bias in the distance-related variables.
%
Initially, for each problem the position-related bias and distance-related bias ($\theta$) are kept exactly as the one proposed in the original work~\cite{li2016biased}.
%
Then, for each problem its initial distance-related bias value ($\theta$) is iterativelly decreased by a factor of two.
%
Specifically, the distance-related bias taken into account are $\{\theta, \frac{\theta}{2}, \frac{\theta}{4}, \frac{\theta}{8}, \frac{\theta}{16}, \frac{\theta}{32}, \frac{\theta}{64}, \frac{\theta}{128}, \frac{\theta}{256}, \frac{\theta}{512}, \frac{\theta}{1028}\}$.
%
Figure~\ref{fig:BT} shows the mean \HV{} ratio obtained with several distance-related biasses. 
%
Also note that the \BT{}2 problem is not taken into consideration due that increasing its bias values provokes numerical instability since that it incorporates a different bias transformation, nevertheless all the results can be consulted in the supplementary document.
%
Taking exactly the original configuration (bias of $\theta$)~\cite{li2016biased} \AVSDMOEAD{} is sigthly better than \MOEADDE{}, but as soon as the bias is decreased to $\frac{\theta}{32}$ the performance of \MOEADDE{} decays aggressively.
%
Furthermore, the performance of \AVSDMOEAD{} is superior than $0.9$ with biasses values upper or equal to $\frac{\theta}{256}$ which is quite superior than the state-of-the-art \MOEAS{} whose values at that point are approximately of $0.75$.
%
Figure~\ref{fig:attainment_BT} shows the 50\% of attainment surface of \BT{}6, \BT{}7 and \BT{}8 with a bias of $\frac{\theta}{32}$.
%
\BT{}6 and \BT{}8 have simple nolinear Pareto set while \BT{}7 has a complicated nolinear Pareto set.
%
\BT8{} is multimodal.
%
Although that \MOEADDE{} converged to a region of the Pareto front with \BT{}6 \AVSDMOEAD{} covered a huge region of the Pareto front, in fact this shows that for this problem promoting diversity in the decision space results in diversity in the objective space.
%
In addition, \AVSDMOEAD{} converges quite well in complicates nonlinear Pareto sets shown in the 50\% attained surface of \BT{}7 (Figure~\ref{fig:attainment_BT}).
%
Finally but not less important \AVSDMOEAD{} shows a superior behaviour with biased and multimodal problems as is the case of \BT{}8 whose attainment surfaces have converged much better to the Pareto front.
