
This section describes our proposal, the \textit{Archived Variable Space Diversity \MOEA{} based on Decomposition} (\AVSDMOEAD{})\footnote{\scriptsize The source code in C++ is freely available at \url{https://drive.google.com/drive/folders/1JAIrOybzyafxo2BUW99DnsUiqzZ3Wwco}}.
%
The main novelty and motivation behind \AVSDMOEAD{} is the incorporation of an explicit way to manage diversity in the variable space,
the goal being to improve the behavior in terms of objective space metrics, especially in long-term executions, which is the
environment where diversity-aware techniques have excelled~\cite{segura2015novel}.
%
Although \AVSDMOEAD{} is inspired by \MOEAD{}, it has been simplified, so in some ways it resembles more mature
decomposition-based \MOEAS{}, such as \textsc{wbga}~\cite{Hajela:93}.
%
For instance, the notion of subproblem neighborhood is not used and the dynamic resource allocation usually applied in modern variants
of \MOEAD{} is deactivated.
%
The main reason for the simplification is to show that even a simple \MOEA{} incorporating our design principles can
improve further more complex state-of-the-art algorithms.
%

Our proposal decomposes the \MOP{} into several single-objective problems.
%
Notwithstanding that any scalarization approach can be employed, our strategy applies the \ASF{}, 
which has reported some of the most effective results in recent years~\cite{hernandez2015improved}.
%
Let $\lambda_1, ..., \lambda_N$ be a set of weight vectors and $z^*$ a reference vector.
The \MOP{} is decomposed into $N$ scalar optimization sub-problems as shown in (\ref{eqn:approach}).
%

\begin{equation}\label{eqn:approach}
\displaystyle{
 g^{asf}(x| \lambda_j, z^*) = \max_{ 1 \leq i \leq M} \left \{ \frac{ | f_i(x) - z_i^*|}{\lambda_{j,i}} \right \} 
}
\end{equation}

\begin{algorithm}[!t]
\algsetup{linenosize=\tiny}
        \caption{Main procedure of \AVSDMOEAD{}}
        \begin{small}
\begin{algorithmic}[1]
				\STATE Input: $N$ (Population and Archive Size), $\lambda$ (a set of $N$ weight vectors for decomposition in the population),
				$\Lambda$ (a set of weight vectors for the R2-based archive), $D_I$ (Initial Penalty Threshold), $CR$ (Crossover Rate),
				$F$ (Scale Factor), $p_m$ (mutation probability)
        \STATE Output: $A^{t}$ (Archive with $N$ solutions)

        \STATE \textbf{Initialization}: Generate an initial population $P^0$ with $N$ individuals \label{alg_1:1}
%        \STATE Let $\lambda = \{\lambda_1, ..., \lambda_N \}$ be a set of evenly spread weight vectors \label{alg_1:2}
        \STATE \textbf{Evaluation}: Evaluate each individual in $P^0$ and assemble the reference vector $z^*$ with the best objective values \label{alg_1:3}
				\STATE \textbf{Archive Initialization:} $A^0 = P^0$
        \STATE Assign $t=0$ \label{alg_1:4}
        \WHILE{ (not stopping criterion)  } \label{alg_1:5}
           \FOR{ each individual $P_i^t \in P^t$} \label{alg_1:6}
    %           \STATE \textbf{Mating selection}: Select randomly three indexes ($r_1 \neq r_2 \neq r_3 \neq i$) from the entire population. \label{alg_1:7}
               \STATE \textbf{DE variation}: Generate solution $Q^t_{i}$ by applying DE/rand/1/bin with $F$ as the mutation scale factor and $CR$ as the crossover rate, using $P_{i}^t$ as the target vector \label{alg_1:8}
							 \STATE \textbf{Mutation}: Apply polynomial mutation to $Q^t_{i}$ with probability $p_m$
               \STATE \textbf{Evaluation}: Evaluate the new individual $Q^t_{i}$ and update the reference vector $z^*$ with the best objective values. \label{alg_1:9}
           \ENDFOR \label{alg_1:10}
           \STATE \textbf{Survivor selection}: Generate $P^{t+1}$ by applying the replacement scheme described in Algorithm \ref{alg:replacement}, using $P^t$, $Q^t$, $\lambda$, $z^*$ and $D_I$ as input \label{alg_1:11}
	   \STATE \textbf{Update Archive}: Create $A^{t+1}$ by applying Algorithm~\ref{alg:r2_Indicator} using $A^{t}$, $Q^t$, $\Lambda$, and $z^*$ as input. \label{alg_1:11bis}
           \STATE $t=t+1$ \label{alg_1:12}
        \ENDWHILE \label{alg_1:13}
        \RETURN $A^{t}$ 
        \end{algorithmic}
        \end{small}
\label{alg:vsd-moead}
\end{algorithm}


The main novelty of \AVSDMOEAD{} appears in the survivor selection scheme.
%
In keeping with some of the most successful single-objective diversity-aware algorithms~\cite{segura2016improving}, the 
replacement strategy relates the degree of diversity in the variable space to the stopping criterion
and elapsed generations.
%
The aim of this relationship is to gradually bias the search from exploration to exploitation as the
optimization evolves.
%
Specifically, diversity is explicitly promoted less and less until half of the total generations. 
%
Then, in the remaining generations, \AVSDMOEAD{} behaves similarly to most popular
\MOEAS{}, i.e. the diversity in the variable space is not considered explicitly.

The main procedure of \AVSDMOEAD{} is shown in Algorithm~\ref{alg:vsd-moead}.
%
Its general template is quite standard.
%
The variation step is similar to those used in typical \MOEAS{}, meaning
it is based on crossover and mutation and any operators might be used.
%
Specifically, $N$ individuals are created in each generation by randomly selecting at each step three individuals
to apply the $DE/rand/1/bin$ operator.
%
Note that each member of the population is used as the target vector once.
%
Then, polynomial mutation is applied to the output of the $DE$ operator.
%
As in most current \MOEAD{} variants, the initial population is generated randomly,
the number of weight vectors ($|\lambda|$) is equal to the population size,
and the reference vector $z^*$ used for \ASF{} consists of the best 
objective values achieved.
%
The weight vectors used in this paper are based on an uniform design strategy and are
specified in the experimental validation section.
%
Finally, the survivor selection stage is applied.
%
This is quite different from traditional techniques, in the sense that $P^t$ and $Q^t$ are merged, meaning
that unlike in \MOEAD{}, the position of each individual is not important, and a diversity-aware
selection is performed.
%
Since this is the novelty of the paper, its working operation is given in detail.

\begin{algorithm}[!t]
\algsetup{linenosize=\tiny}
        \caption{Procedure to update the R2-based archive}
        \begin{small}
\begin{algorithmic}[1]
	\STATE Input: $A^t$ (External archive in the current generation), $Q^t$ (Offspring of current generation), $\Lambda$ (weight vectors for $R2$), $z^*$ (Reference vector)
	\STATE Output: $A^{t+1}$
	\STATE $R^t= A^t \cup Q^t$
	\STATE $A^{t+1} = \emptyset$
	\WHILE{ $|A^{t+1}| < N$ }
	\STATE $\forall x \in R^t : r(x) = R2(A^{t+1} \cup \{x\}; \Lambda, z^*)$
	\STATE $x^* = argmin(r(x):x \in R^t)$ 
	\STATE $A^{t+1} = A^{t+1} \cup \{x^*\}$
	\STATE $R^t = R^t \setminus \{ x^* \}$ 
  	\ENDWHILE
        \end{algorithmic}
        \end{small}
\label{alg:r2_Indicator}
\end{algorithm}
\begin{algorithm}[!t]
\algsetup{linenosize=\tiny}
        \caption{Replacement Phase of \AVSDMOEAD{}}
\begin{small}
\begin{algorithmic}[1]
\STATE Input: $P^t$ (Population of current generation), $Q^t$ (Offspring of current generation), $\lambda$ (a set of weight vectors), $z^*$ (Reference vector), and $D_I$ (Initial Penalty Threshold).
        \STATE Output: $P^{t+1}$
        \STATE $R^t = P^t \cup Q^t$\label{alg_2:1} 
        \STATE $P^{t+1} = \emptyset$ \label{alg_2:2}
        \STATE $Penalized = \emptyset$ \label{alg_2:3}
				\STATE $r\lambda = \lambda$
        \STATE $D^t = D_I - D_I * \frac{G_{Elapsed}}{0.5*G_{End}}$ \label{alg_2:5} 
        \WHILE{ $|P^{t+1}| <  N$} \label{alg_2:6}
            \STATE Compute $DCS$ in $R^t$ using $P^{t+1}$ as reference set \label{alg_2:7}
            \STATE Move the individuals in $R^t$ with $DCS < D^t$ to $Penalized$ \label{alg_2:8}
%	   \STATE Compute the diversity-contribution of each candidate $i \in R^t$ to the survivor set $P^{t+1}$\label{alg:7}
%	   \STATE Move the crowdest individuals from $R^t$ to $Penalized$; Those individuals whose diversity-contribution is less than the threshold $D^t$\label{alg:8}
                \IF{$R^t$ is empty} \label{alg_2:9}
                    \STATE Compute $DCS$ of each individual in $Penalized$ set employing $P^{t+1}$ as reference set \label{alg_2:10}
                    \STATE Move the individual in $Penalized$ with the largest $DCS$ to $R^t$ \label{alg_2:11}
%		    \STATE Compute the diversity-contribution of each individual in $Penalized$ to the survivor set $P^{t+1}$\label{alg:10}
%                    \STATE Move the most suitable individual from $Penalized$ to the survivor set $R^t$; the one with the highest diversity-contribution to $R^t$ \label{alg:11}
                \ENDIF \label{alg_2:12}
            \STATE Identify the pair of non-penalized individual $R_i^t$ and weight vector $r\lambda_j$ with the best scalarizing function value according to $g^{asf}(R_i^t | r\lambda_j, z^*)$ \label{alg_2:13}
%	    \STATE $\displaystyle{ R_i^t, \lambda_i = \max_{k \in |R^t|, l \in |\Lambda|} g(R_k^t | \lambda_l, \mathbf{z})}$ 
	    \STATE Move individual $R_i^t$ to $P^{t+1}$ \label{alg_2:14}
            \STATE Erase the weight vector r$\lambda_j$ from r$\lambda$ \label{alg_2:15}
        \ENDWHILE \label{alg_2:16}
        \RETURN $P^{t+1}$ \label{alg_2:17}
        \end{algorithmic}
\end{small}
\label{alg:replacement}
\end{algorithm}



Note that \AVSDMOEAD{} incorporates an external archive to store the best solutions.
%
While in \MOEAD{} this is considered optional, in our approach it is quite important because the penalty approach
included to pick up the survivors of the population in the replacement phase might discard elite solutions for some weight vectors.
%
Since methods based on the R2 indicator~\cite{trautmann2013r2} have reported quite high-quality solutions, our
archive is based on the R2 indicator applying the \ASF{} and the weights $\Lambda$ to generate the utility functions.
%
In each iteration, the archive selects $N$ candidate solutions by combining its contents with the offspring of each 
generation (see line~\ref{alg_1:11bis} in Algorithm~\ref{alg:vsd-moead}).
%
This is done by following a simple greedy approach (Algorithm~\ref{alg:r2_Indicator}).
%
Specifically, it iteratively selects the individual that minimizes the $R2$ (lines 6-9) considering
the individuals already selected with ties broken randomly.
%
Note that since $R2$ is weakly Pareto-compliant, it might happen that some non-dominated individuals do not contribute
to $R2$, so incorporating more complex archiving strategies might be helpful.
%
However, our preliminary experiments have shown that this it not overly important for proper performance, so we decided to maintain
its simplicity.
%

%
\subsection{Novel Replacement Phase of \AVSDMOEAD{} }

The purpose of the replacement phase (see Algorithm~\ref{alg:replacement}) is to select the set of survivors of the next generation.
%
The survivor selection described in this work incorporates similar design principles to those applied in 
the diversity-aware single-objective optimizer \textsc{de-edm}~\cite{castillo2019differential}.
%
It operates as follows.
%
First, the parent and offspring populations are merged in a multi-set to establish the candidate set $R^t$ (line 3).
%
A key of the scheme is to promote the selection of individuals with a large enough contribution to diversity
in the variable space.
%
Specifically, the contribution of an individual $x$ is calculated as $\displaystyle{\min_{p \in P^{t+1}}\ Distance(x, p)}$, 
where $P^{t+1}$ is the multi-set of the already picked survivors and the normalized Euclidean distance
specified in (\ref{eqn:distance}) is applied.
%
Note that in the pseudocode, the tag \DCS{} (Distance to Closest Solution) is used to denote the contribution to diversity.

\begin{equation}\label{eqn:distance}
Distance(A, B) =   \left ( \frac{1}{D}  \sum_{i=1}^D \left ( \frac{A_i - B_i}{x_i^{(U)} - x_i^{(L)}} \right )^2  \right)^{1/2}
\end{equation}

In order to promote the selection of distant individuals, a threshold $D^t$ is dynamically calculated (line 7) and 
individuals with a $DCS$ value lower than the threshold are considered as undesirable individuals.
%
Note that the calculation of $D^t$ depends on an initial penalty threshold ($D_I$), which is a parameter of our proposal,
on the number of generations that have evolved ($G_{Elapsed}$) and on the stopping criterion ($G_{End}$), i.e., the number of
generations to evolve.
%
Specifically, the value is decreased linearly as generations evolve.
%
Since survivors with larger \DCS{} values provoke exploration steps, while survivors with short \DCS{} values promote
intensification steps, this linear decrease promotes
a gradual transition from exploration to exploitation.
%
Also note that after $50\%$ of the total number of generations, the $D^t$ value is below 0, 
meaning that no penalties are applied and a more traditional strategy focused only on the objective values
is used to perform the selection steps.

The strategy iteratively selects an individual from the candidate set ($R^t$) to enter the new population ($P^{t+1}$) until
it is filled with $N$ individuals (lines 8-16).
%
In particular, the aim is to select a proper individual for each weight vector, while at the same time fulfilling
the condition imposed for the contribution to diversity in the variable space.
%
In order to satisfy this last condition, non-selected individuals with a $DCS$
lower than $D^t$ are moved from $R^t$ to the $Penalized$ set (lines 9-10), and in each iteration
an individual belonging to $R^t$ is selected to survive.
%
The set of weight vectors considered by our strategy are initially placed in $r\lambda$.
%
In each iteration, the individual in $R^t$ with the best scalarizing function for any of the weight vectors in
$r\lambda$ is identified (line 14).
%
Then, this individual is selected as a survivor (line 15) and the weight vector used is erased
from $r\lambda$ (line 16).
%
Note that $N$ individuals are selected, meaning that each weight vector is used to select exactly one individual.
%
Also note that it might happen that $R^t$ is empty prior to selecting $N$ individuals.
%
This means that the diversity is lower than expected, so
in order to increase the amount of exploration, the individual with the largest \DCS{} value in
the $Penalized$ set is selected to survive (lines 11 - 13).
